{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78565\n"
     ]
    }
   ],
   "source": [
    "f = open('./corpus.txt','r', encoding='utf-8')\n",
    "data = [line[:-1] for line in f.readlines()]\n",
    "f.close()\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel = list('aAâÂăĂèÈềỀiIóÓốỐớỚùÙáÁấẤắẮẹẸệỆíÍòÒồỒờỜủỦàÀầẦằẰẽẼễỄìÌọỌộỘợỢũŨạẠậẬặẶẻẺểỂịỊõÕỗỖỡỠụỤãÃẫẪẵẴeEêÊĩĨỏỎổỔởỞuUưƯảẢẩẨẳẲéÉếẾỉỈoOôÔơƠúÚứỨừỪýÝỵỴửỬỳỲữỮỷỶựỰyYỹỸ')\n",
    "fullLetters = vowel + list('bBcCkKqQfFjJrRdDlLsSđĐmMtTgGnNhHpPvVwWzZxX')\n",
    "\n",
    "#Những lỗi thường gặp trong tiếng việt\n",
    "typo = {'á':'as','Á':'As','à':'af','À':'Af','ả':'ar','Ả':'Ar','ã':'ax','Ã':'Ax','ạ':'aj','Ạ':'Aj',\n",
    "        'ă':'aw','Ă':'Aw','ắ':'aws','Ắ':'Aws','ằ':'awf','Ằ':'Awf','ẳ':'awr','Ẳ':'Awr','ẵ':'awx','Ẵ':'Awx','ặ':'awj','Ặ':'Awj',\n",
    "        'â':'aa','Â':'Aa','ầ':'aaf','Ầ':'Aaf','ấ':'aas','Ấ':'Aas','ẩ':'aar','Ẩ':'Aar','ẫ':'aax','Ẫ':'Aax','ậ':'aaj','Ậ':'Aaj',\n",
    "        'é':'es','É':'Es','è':'ef','È':'Ef','ẻ':'er','Ẻ':'Er','ẽ':'ex','Ẽ':'Ex','ẹ':'ej','Ẹ':'Ej',\n",
    "        'ề':'eef','Ề':'Eef','ế':'ees','Ế':'Ees','ể':'eer','Ể':'Eer','ễ':'eex','Ễ':'Eex','ệ':'eej','Ệ':'Eej','ê':'ee','Ê':'Ee',\n",
    "        'ì':'if','Ì':'If','í':'is','Í':'Is','ỉ':'ir','Ỉ':'Ir','ĩ':'ix','Ĩ':'Ix','ị':'ij','Ị':'Ij',\n",
    "        'ò':'of','Ò':'Of','ó':'os','Ó':'Os','ỏ':'or','Ỏ':'Or','õ':'ox','Õ':'Ox','ọ':'oj','Ọ':'Oj',\n",
    "        'ô':'oo','Ô':'Ô','ồ':'oof','Ồ':'Oof','ố':'oos','Ố':'Oos','ổ':'oor','Ổ':'Oor','ỗ':'oox','Ỗ':'Oox','ộ':'ooj','Ộ':'Ooj',\n",
    "        'ơ':'ow','Ơ':'Ow','ờ':'owf','Ờ':'Owf','Ớ':'ows','Ớ':'Ows','ở':'owr','Ở':'Owr','ỡ':'owx','Ỡ':'Owx','ợ':'owj','Ợ':'Owj',\n",
    "        'ù':'uf','Ù':'Uf','ú':'us','Ú':'Us','ủ':'ur','Ủ':'Ur','ũ':'ux','Ũ':'Ux','ụ':'uj','Ụ':'Uj',\n",
    "        'ư':'uw','Ư':'Uw','ừ':'uuf','Ừ':'Uwf','ứ':'uus','Ứ':'Uws','ử':'uwr','Ử':'Uwr','ữ':'uwx','Ữ':'Uwx','ự':'uwj','Ự':'Uwj',\n",
    "        'ý':'ys','Ý':'Ys','ỳ':'yf','Ỳ':'Yf','ỷ':'yr','Ỷ':'Yr','ỹ':'yx','Ỹ':'Yx','ỵ':'yj','Ỵ':'Yj','đ':'dd','Đ':'DD'}\n",
    "\n",
    "region = {'ả':'ã','ã':'ả','ẻ':'ẽ','ẽ':'ẻ','ỉ':'ĩ','ĩ':'ỉ','ỏ':'õ','õ':'ỏ','ủ':'ũ','ũ':'ủ','ỷ':'ỹ','ỹ':'ỷ','ử':'ữ','ữ':'ử','ẳ':'ẵ','ẵ':'ẳ','ẩ':'ẫ','ẫ':'ẩ','ể':'ễ','ễ':'ể','ổ':'ỗ','ỗ':'ổ','ở':'ỡ','ỡ':'ở','ử':'ữ','ữ':'ử',\n",
    "          'Ả':'Ã','Ã':'Ả','Ẻ':'Ẽ','Ẽ':'Ẻ','Ỉ':'Ĩ','Ĩ':'Ỉ','Ỏ':'Õ','Õ':'Ỏ','Ủ':'Ũ','Ũ':'Ủ','Ỷ':'Ỹ','Ỹ':'Ỷ','Ử':'Ữ','Ữ':'Ử','Ẳ':'Ẵ','Ẵ':'Ẳ','Ẩ':'Ẫ','Ẫ':'Ẩ','Ể':'Ễ','Ễ':'Ể','Ổ':'Ỗ','Ỗ':'Ổ','Ở':'Ỡ','Ỡ':'Ở','Ử':'Ữ','Ữ':'Ử'}\n",
    "\n",
    "region2 = {'ch':'tr','tr':'ch','gi':'d','d':'gi','l':'n','n':'l','x':'s','s':'x',\n",
    "           'CH':'TR','TR':'CH','GI':'D','D':'GI','L':'N','N':'L','X':'S','S':'X'}\n",
    "\n",
    "acronym = {'con trai':'ctrai','không':'khôg','bố mẹ':'bme','chúng ta':'cta','mình':'mih','mối quan hệ':'mqh','con gái':'cgai','những':'nhưg','mọi người':'mn','sinh viên tình nguyện':'svtn','rồi':'r','quan tâm':'qt',\n",
    "           'thương':'thươg','chung':'chug','trường':'trươg','thôi':'thoy','đăng ký':'đk','ảo tưởng sức mạnh':'atsm','anh':'a','biết':'bít','chồng':'ck','em':'e','gì':'j','giờ':'h','không':'k','muốn':'mún','ông':'ôg',\n",
    "           'phải':'fai','tôi':'tui','vợ':'vk','yêu':'iu','thôi':'thui','tình':'tìh','cái':'kai','á':'ak','đồng':'đồg','đông':'đôg','Con Trai':'Ctrai','Không':'Khôg','Bố Mẹ':'Bme','Chúng Ta':'Cta','Mình':'Mih',\n",
    "           'Mối Quan Hệ':'Mqh','Con Gái':'Cgai','Những':'Nhưg','Mọi Người':'Mn','Sinh Viên Tình Nguyện':'Svtn','Rồi':'r','Quan Tâm':'Qt','Thương':'Thươg','Chung':'Chug','Trường':'Trươg','Thôi':'Thoy','Đăng Ký':'Đk',\n",
    "           'Ảo Tưởng Sức Mạnh':'Atsm','Anh':'a','Biết':'Bít','Chồng':'Ck','Em':'e','Gì':'j','Giờ':'h','Không':'k','Muốn':'Mún','Ông':'Ôg','Phải':'Fai','Tôi':'Tui','Vợ':'Vk','Yêu':'Iu',\n",
    "           'Thôi':'Thui','Tình':'Tìh','Cái':'Kai','á':'Ak','Đồng':'Đồg','Đông':'Đôg','CON TRAI':'CTRAI','KHÔNG':'KHÔG','BỐ MẸ':'BME','CHÚNG TA':'CTA','MÌNH':'MIH','MỐI QUAN HỆ':'MQH','CON GÁI':'CGAI','NHỮNG':'NHƯG',\n",
    "           'MỌI NGƯỜI':'MN','SINH VIÊN TÌNH NGUYỆN':'SVTN','RỒI':'R','QUAN TÂM':'QT','THƯƠNG':'THƯƠG','CHUNG':'CHUG','TRƯỜNG':'TRƯƠG','THÔI':'THOY','ĐĂNG KÝ':'ĐK','ẢO TƯỞNG SỨC MẠNH':'ATSM','ANH':'A','BIẾT':'BÍT',\n",
    "           'CHỒNG':'CK','EM':'E','GÌ':'J','GIỜ':'H','KHÔNG':'K','MUỐN':'MÚN','ÔNG':'ÔG','PHẢI':'FAI','TÔI':'TUI','VỢ':'VK','YÊU':'IU','THÔI':'THUI','TÌNH':'TÌH','CÁI':'KAI','Á':'AK','ĐỒNG':'ĐỒG','ĐÔNG':'ĐÔG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "def teenCode(sentence):\n",
    "    random = np.random.uniform(0,1,1)[0]\n",
    "    newSentence = str(sentence)\n",
    "\n",
    "    if random > 0.5:\n",
    "        for word in acronym.keys():\n",
    "            if re.search(word, newSentence):\n",
    "                random2 = np.random.uniform(0,1,1)[0]\n",
    "                if random2 < 0.5:\n",
    "                    newSentence = re.sub(word, acronym[word], newSentence)\n",
    "        return newSentence\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def addNoise(sentence):\n",
    "    sentence = teenCode(sentence)\n",
    "    noisySentence = ''\n",
    "\n",
    "    i=0\n",
    "    while i < len(sentence):\n",
    "        if sentence[i] not in fullLetters:\n",
    "            noisySentence += sentence[i]\n",
    "        else:\n",
    "            random = np.random.uniform(0,1,1)[0]\n",
    "            if random <= 0.94:\n",
    "                noisySentence += sentence[i]\n",
    "            elif random <= 0.985:\n",
    "                if sentence[i] in typo.keys():\n",
    "                    if sentence[i] in region.keys():\n",
    "                        random2 = np.random.uniform(0,1,1)[0]\n",
    "                        if random2 <= 0.4:\n",
    "                            noisySentence += typo[sentence[i]]\n",
    "                        elif random2 < 0.8:\n",
    "                            noisySentence += region[sentence[i]]\n",
    "                        elif random2 < 0.95:\n",
    "                            noisySentence += unidecode(sentence[i])\n",
    "                        else:\n",
    "                            noisySentence += sentence[i]\n",
    "                    else:\n",
    "                        random3 = np.random.uniform(0,1,1)[0]\n",
    "                        if random3 <= 0.6:\n",
    "                            noisySentence += typo[sentence[i]]\n",
    "                        elif random3 < 0.9:\n",
    "                            noisySentence += unidecode(sentence[i])\n",
    "                        else:\n",
    "                            noisySentence += sentence[i]\n",
    "                elif i == 0 or sentence[i-1] not in fullLetters:\n",
    "                    random4 = np.random.uniform(0,1,1)[0]\n",
    "                    if random4 <= 0.9:\n",
    "                        if i < len(sentence) - 1 and sentence[i] in region2.keys() and sentence[i+1] in vowel:\n",
    "                            noisySentence += region2[sentence[i]]\n",
    "                        elif i < len(sentence) - 2 and sentence[i:i+2] in region2.keys() and sentence[i+2] in vowel:\n",
    "                            noisySentence += region2[sentence[i:i+2]]\n",
    "                            i+=1\n",
    "                        else:\n",
    "                            noisySentence += sentence[i]\n",
    "                    else:\n",
    "                        noisySentence += sentence[i]\n",
    "                else:\n",
    "                    noisySentence += sentence[i]\n",
    "            else:\n",
    "                newRandom = np.random.uniform(0,1,1)[0]\n",
    "                if newRandom <= 0.33 and i != len(sentence) - 1:\n",
    "                    noisySentence += sentence[i+1]\n",
    "                    noisySentence += sentence[i]\n",
    "                    i += 1\n",
    "                else:\n",
    "                    noisySentence += sentence[i]\n",
    "        i += 1\n",
    "\n",
    "    return noisySentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ['\\x00',' '] + list('0123456789') + fullLetters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "\n",
    "for text in data:\n",
    "    for c in set(text):\n",
    "        if re.match('\\w', c) and c not in alphabet:\n",
    "            uc = unidecode(c)\n",
    "            if re.match('\\w', uc) and uc not in alphabet:\n",
    "                text = re.sub(c, '', text)\n",
    "            else:\n",
    "                text = re.sub(c, uc, text)\n",
    "    phrases += re.findall(r'\\w[\\w\\s]+', text)\n",
    "\n",
    "phrases = [p.strip() for p in phrases if len(p.split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "NGRAM = 5\n",
    "MAXLEN = 39\n",
    "\n",
    "listNGRAMS = []\n",
    "for p in phrases:\n",
    "    listP = p.split()\n",
    "    if (len(listP) >= NGRAM):\n",
    "        for ngr in ngrams(p.split(), NGRAM):\n",
    "            if len(' '.join(ngr)) <= MAXLEN:\n",
    "                listNGRAMS.append(' '.join(ngr))\n",
    "    elif len(' '.join(listP)) <= MAXLEN:\n",
    "        listNGRAMS.append(' '.join(listP))\n",
    "\n",
    "listNGRAMS = list((listNGRAMS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderData(text):\n",
    "    x = np.zeros((MAXLEN, len(alphabet)))\n",
    "    for i,c in enumerate(text[:MAXLEN]):\n",
    "        x[i, alphabet.index(c)] = 1\n",
    "    if i < MAXLEN - 1:\n",
    "        for j in range(i+1, MAXLEN):\n",
    "            x[j, 0] = 1\n",
    "    return x\n",
    "\n",
    "def decoderData(x):\n",
    "    x = x.argmax(axis=-1)\n",
    "    return ''.join(alphabet[i] for i in x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, TimeDistributed, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTM(256, input_shape = (MAXLEN, len(alphabet)), return_sequences= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 39, 256)           465920    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 39, 512)          1050624   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 39, 256)          131328    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 39, 256)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 39, 198)          50886     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 39, 198)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,698,758\n",
      "Trainable params: 1,698,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(encoder)\n",
    "model.add(decoder)\n",
    "model.add(TimeDistributed(Dense(256)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(TimeDistributed(Dense(len(alphabet))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, valid_data = train_test_split(listNGRAMS, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 10\n",
    "\n",
    "def generateData(data, batchSize):\n",
    "    currentIndex = 0\n",
    "    while True:\n",
    "        x, y = [], []\n",
    "        for i in range(batchSize):\n",
    "            y.append(encoderData(data[currentIndex]))\n",
    "            x.append(encoderData(addNoise(data[currentIndex])))\n",
    "            currentIndex += 1\n",
    "            if currentIndex > len(data) - 1:\n",
    "                currentIndex = 0\n",
    "        yield (np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGenerator = generateData(train_data, batchSize=BATCH_SIZE)\n",
    "validationGenerator = generateData(valid_data, batchSize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-12c4bd432120>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  K = model.fit_generator(trainGenerator, epochs=EPOCHS,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2018/2018 [==============================] - 5863s 3s/step - loss: 0.3498 - accuracy: 0.9224 - val_loss: 0.0525 - val_accuracy: 0.9863\n",
      "Epoch 2/10\n",
      "2018/2018 [==============================] - 5929s 3s/step - loss: 0.0420 - accuracy: 0.9889 - val_loss: 0.0303 - val_accuracy: 0.9920\n",
      "Epoch 3/10\n",
      "2018/2018 [==============================] - 5768s 3s/step - loss: 0.0277 - accuracy: 0.9926 - val_loss: 0.0225 - val_accuracy: 0.9939\n",
      "Epoch 4/10\n",
      "2018/2018 [==============================] - 5758s 3s/step - loss: 0.0213 - accuracy: 0.9942 - val_loss: 0.0196 - val_accuracy: 0.9945\n",
      "Epoch 5/10\n",
      "2018/2018 [==============================] - 5827s 3s/step - loss: 0.0180 - accuracy: 0.9951 - val_loss: 0.0146 - val_accuracy: 0.9960\n",
      "Epoch 6/10\n",
      "2018/2018 [==============================] - 5893s 3s/step - loss: 0.0156 - accuracy: 0.9957 - val_loss: 0.0135 - val_accuracy: 0.9963\n",
      "Epoch 7/10\n",
      "2018/2018 [==============================] - 6007s 3s/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.0126 - val_accuracy: 0.9966\n",
      "Epoch 8/10\n",
      "2018/2018 [==============================] - 5866s 3s/step - loss: 0.0128 - accuracy: 0.9965 - val_loss: 0.0117 - val_accuracy: 0.9968\n",
      "Epoch 9/10\n",
      "2018/2018 [==============================] - 6066s 3s/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.0108 - val_accuracy: 0.9970\n",
      "Epoch 10/10\n",
      "2018/2018 [==============================] - 6063s 3s/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0101 - val_accuracy: 0.9972\n"
     ]
    }
   ],
   "source": [
    "K = model.fit_generator(trainGenerator, epochs=EPOCHS, \n",
    "                        steps_per_epoch=len(train_data) // BATCH_SIZE,\n",
    "                        validation_data=validationGenerator,\n",
    "                        validation_steps=len(valid_data) // BATCH_SIZE)\n",
    "\n",
    "model.save('model_{0:.4f}.h5'.format(K.history['val_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2daa6c88faa66e226c755852904ef6f3c7f43da7c34cc68791ac993bb048e8a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
